{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# FeatureEngineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- 문제 : dataset의 큰 용량 때문에 컴퓨터가 연속된 연산을 못하고 계속 퍼지는 현상이 발생.\n",
    "- 해결 : Feature Engineering을 독립적으로 수행해서 그 결과값을 .csv 파일로 만들어 놓음.\n",
    "- 이를 통해 연산을 분리해서 컴퓨터의 부담을 경감시킴.\n",
    "- 앞서의 holidays_events, oil dataset에 대한 feature engineering 의 결과를 적용하고,\n",
    "- train / oil / items / holidays_events / stores dataset들을 merge하여 modeling에 바로 사용할 수 있는 dataset 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 5s, sys: 6.35 s, total: 1min 11s\n",
      "Wall time: 1min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "\n",
    "dtypes = {'store_nbr': np.dtype('int8'),\n",
    "          'id': np.dtype('int32'),\n",
    "          'item_nbr': np.dtype('int32'),\n",
    "          'unit_sales': np.dtype('float64')}\n",
    "\n",
    "df_train = pd.read_csv('./data/train_drop_pro.csv', dtype=dtypes)\n",
    "df_test = pd.read_csv('./data/test.csv', dtype=dtypes)\n",
    "df_oil = pd.read_csv('./data/oil.csv')\n",
    "df_items = pd.read_csv('./data/items.csv')\n",
    "df_holidays = pd.read_csv('./data/holidays_events.csv')\n",
    "df_stores = pd.read_csv('./data/stores.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.68 s, sys: 25.3 ms, total: 2.7 s\n",
      "Wall time: 2.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Replace missing_values in oil_data\n",
    "# reference : https://www.kaggle.com/kaggleslayer/grocery-prediction-with-neural-network\n",
    "min_oil_date = min(df_train.date)\n",
    "max_oil_date = max(df_test.date)\n",
    "\n",
    "calendar = []\n",
    "\n",
    "d1 = datetime.datetime.strptime(min_oil_date, '%Y-%m-%d')\n",
    "d2 = datetime.datetime.strptime(max_oil_date, '%Y-%m-%d')\n",
    "\n",
    "delta = d2 - d1\n",
    "\n",
    "for i in range(delta.days + 1):\n",
    "    calendar.append(datetime.date.strftime(d1 + timedelta(days=i), '%Y-%m-%d'))\n",
    "    \n",
    "calendar = pd.DataFrame({'date':calendar})\n",
    "\n",
    "df_oil = calendar.merge(df_oil, left_on='date', right_on='date', how='left')\n",
    "\n",
    "na_index_oil = df_oil[df_oil['dcoilwtico'].isnull() == True].index.values\n",
    "na_index_oil_plus = na_index_oil.copy()\n",
    "na_index_oil_minus = np.maximum(0, na_index_oil - 1)\n",
    "\n",
    "for i in range(len(na_index_oil)):\n",
    "    k = 1\n",
    "    while (na_index_oil[min(i+k, len(na_index_oil)-1)] == na_index_oil[i]+k):\n",
    "        k += 1\n",
    "    na_index_oil_plus[i] = min(len(df_oil)-1, na_index_oil_plus[i] + k)\n",
    "    \n",
    "for i in range(len(na_index_oil)):\n",
    "    if (na_index_oil[i] == 0):\n",
    "        df_oil.loc[na_index_oil[i], 'dcoilwtico'] = df_oil.loc[na_index_oil_plus[i], 'dcoilwtico']\n",
    "\n",
    "    elif (na_index_oil[i] == len(df_oil)):\n",
    "        df_oil.loc[na_index_oil[i], 'dcoilwtico'] = df_oil.loc[na_index_oil_minus[i], 'dcoilwtico']\n",
    "    \n",
    "    else:\n",
    "        df_oil.loc[na_index_oil[i], 'dcoilwtico'] = (df_oil.loc[na_index_oil_plus[i], 'dcoilwtico'] + df_oil.loc[na_index_oil_minus[i], \n",
    "                                                                       'dcoilwtico'])/2\n",
    "\n",
    "# About earthquake    \n",
    "df_oil['earthquake'] = 0\n",
    "df_oil['earthquake'].loc[746] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.4 ms, sys: 110 µs, total: 12.5 ms\n",
      "Wall time: 12.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# convert holidays_event\n",
    "df_holidays_sort=df_holidays.sort_values(by='locale')\n",
    "\n",
    "df_holidays_L = df_holidays_sort.loc[:66]\n",
    "df_holidays_N = df_holidays_sort.loc[227:154]\n",
    "df_holidays_R = df_holidays_sort.loc[334:278]\n",
    "# dropping datasets which was duplicated on 'date'\n",
    "df_holidays_N = df_holidays_N.drop([35, 40, 156, 235, 242, 245])\n",
    "\n",
    "# description 의 뒷부분 제거\n",
    "def desc_transfer(data_set):\n",
    "    desc_use = data_set['description'].str.split(' ').str[0]\n",
    "    df_base = data_set.drop('description', axis=1)\n",
    "    df_desc_transfered = pd.concat([df_base, desc_use], axis=1)\n",
    "    return df_desc_transfered\n",
    "\n",
    "df_holidays_L_use = desc_transfer(df_holidays_L)\n",
    "df_holidays_N_use = desc_transfer(df_holidays_N)\n",
    "df_holidays_R_use = desc_transfer(df_holidays_R)\n",
    "\n",
    "# holidays 에서 사용할 데이터만 정리\n",
    "df_h_n = df_holidays_N_use[['date', 'type', 'description']]\n",
    "df_h_r = df_holidays_R_use[['date', 'locale_name', 'description']]\n",
    "df_h_l = df_holidays_L_use[['date', 'locale_name', 'description']]\n",
    "\n",
    "# national의 description에서 장기연휴의 경우 뒤에 붙어 있는 +/- 를 제거하기\n",
    "drop_add_plus = df_h_n['description'].str.split('+').str[0]\n",
    "drop_add_base_p = df_h_n.drop('description', axis=1)\n",
    "df_h_n_drop_plus = pd.concat([drop_add_base_p, drop_add_plus], axis=1)\n",
    "\n",
    "drop_add_minus = df_h_n_drop_plus['description'].str.split('-').str[0]\n",
    "drop_add_base_m = df_h_n_drop_plus.drop('description', axis=1)\n",
    "df_h_n_drop = pd.concat([drop_add_base_m, drop_add_minus], axis=1)\n",
    "\n",
    "# locale_name을 regional 과 locale 에 따라 구분하기 편하게 변경함.\n",
    "df_h_r_rename = df_h_r.rename(index=str, columns={\"locale_name\": \"state\"})\n",
    "df_h_l_rename = df_h_l.rename(index=str, columns={\"locale_name\": \"city\"})\n",
    "\n",
    "# merge 과정에서 df_h_l_rename의 2016-07-24 이 city와 date가 중복됨을 확인함. 둘중 하나를 지움.\n",
    "df_h_l_rename_drop = df_h_l_rename.drop(['265'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merge datasets\n",
    "def merge_df(dataframe):\n",
    "    df_1 = pd.merge(dataframe, df_items, how='left', on='item_nbr')\n",
    "    df_2 = pd.merge(df_1, df_stores, how='left', on='store_nbr')\n",
    "    df_3 = pd.merge(df_2, df_oil, how='left', on='date')\n",
    "    df_4 = pd.merge(df_3, df_h_n_drop, how='left', on='date', suffixes=('_store', '_holiday'))\n",
    "    df_5 = pd.merge(df_4, df_h_r_rename, how='left', on=['date', 'state'], suffixes=('', '_r'))\n",
    "    df_merged = pd.merge(df_5, df_h_l_rename_drop, how='left', on=['date', 'city'], suffixes=('_n', '_l'))\n",
    "\n",
    "    having_Nan_Columns = ('type_holiday', 'description_n', 'description_r', 'description_l')\n",
    "\n",
    "    for Column in having_Nan_Columns:\n",
    "        df_merged[Column] = df_merged[Column].fillna('None')\n",
    "    \n",
    "    return df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert date feature\n",
    "def convert_date(df):\n",
    "    df['year'] = df.date.apply(lambda x: x.split('-')[0])\n",
    "    df['month'] = df.date.apply(lambda x: x.split('-')[1])\n",
    "    df['day'] = df.date.apply(lambda x: x.split('-')[2])\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['week_day'] = df['date'].dt.weekday_name\n",
    "    df = df.drop('date', axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.2 s, sys: 1.98 s, total: 12.2 s\n",
      "Wall time: 12.2 s\n"
     ]
    }
   ],
   "source": [
    "# Execute functions\n",
    "%%time\n",
    "\n",
    "df_train = merge_df(df_train)\n",
    "df_train = convert_date(df_train)\n",
    "\n",
    "df_test = merge_df(df_test)\n",
    "df_test = convert_date(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.3 s, sys: 173 ms, total: 29.5 s\n",
      "Wall time: 29.6 s\n"
     ]
    }
   ],
   "source": [
    "# Make Dataframes to use in Modeling in .csv file\n",
    "%%time\n",
    "df_train.to_csv('./data/train_use.csv', index=False)\n",
    "df_test.to_csv('./data/test_use.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
